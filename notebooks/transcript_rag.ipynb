{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstuts_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class ApplicationParameters:\n",
    "    filename = \"data/test.json\"\n",
    "    embedding_model = \"text-embedding-3-small\"\n",
    "    n_context_docs = 2\n",
    "\n",
    "params = ApplicationParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "def set_api_key_if_not_present(key_name, prompt_message=\"\"):\n",
    "    if len(prompt_message) == 0:\n",
    "        prompt_message=key_name\n",
    "    if key_name not in os.environ or not os.environ[key_name]:\n",
    "        os.environ[key_name] = getpass.getpass(prompt_message)\n",
    "\n",
    "set_api_key_if_not_present(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "First, we will read in the transcripts of the videos and convert them to Documents\n",
    "with appropriate metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"../data/test.json\"\n",
    "\n",
    "data = json.load(open(filename, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from pstuts_rag.datastore import transcripts_load\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "docs_chunks_semantic = transcripts_load(data,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R - retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hit it with a semantic chunker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "qdrantclient = QdrantClient(\":memory:\")\n",
    "\n",
    "vectorstore = pstuts_rag.datastore.initialize_vectorstore(\n",
    "    client=qdrantclient,\n",
    "    collection_name=f\"{params.filename}_qdrant\",\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "_ = vectorstore.add_documents(documents=docs_chunks_semantic)\n",
    "retriever =vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": params.n_context_docs}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - Augmentation\n",
    "\n",
    "We need to populate a prompt for LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\\\n",
    "You are a helpful and friendly Photoshop expert.\n",
    "\n",
    "Your job is to answer user questions based **only** on transcript excerpts from training videos. These transcripts include **timestamps** that indicate when in the video the information was spoken.\n",
    "\n",
    "The transcript is from **spoken audio**, so it may include informal phrasing, filler words, or fragmented sentences. You may interpret meaning **only to the extent it is clearly implied**, but you must not add new information or invent details.\n",
    "\n",
    "‚úÖ Your Responsibilities\n",
    "\n",
    "1. Use **only** the transcript to answer.\n",
    "2. If a clear answer is **not** present in the transcript, respond exactly:  \n",
    "   \"I don't know. This isn‚Äôt covered in the training videos.\"\n",
    "3. When appropriate, include the **timestamp** of relevant information in your answer to help the user locate it in the original video.\n",
    "4. Do **not** make assumptions or draw on outside knowledge.\n",
    "\n",
    "üí° Style & Formatting Tips\n",
    "\n",
    "- Use a step-by-step format when explaining procedures üìã.\n",
    "- Add relevant emojis for clarity and friendliness üé®üñ±Ô∏èüîß.\n",
    "- Keep your answers short, clear, and conversational.\n",
    "- The input timestamps will be in seconds. When reporting timestamps, convert them into minute:seconds format.\n",
    "\n",
    "‚õî Never Do This\n",
    "\n",
    "- ‚ùå Don't guess or summarize from general knowledge.\n",
    "- ‚ùå Don‚Äôt fabricate steps, names, or features not in the transcript.\n",
    "- ‚ùå Don‚Äôt omit the fallback response when required.\n",
    "\"\"\"),\n",
    "    (\"user\",\"\"\"\\\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "NEVER invent the explanation. ALWAYS use ONLY the context information.\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\n",
    "\"\"\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_references(context):\n",
    "    references = [ \n",
    "                    {k: doc.metadata[k] for k in (\"title\",\"source\",\"start\",\"stop\")} \n",
    "                    for doc in context\n",
    "    ] \n",
    "    print(type(references))\n",
    "    return json.dumps(references,indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "We will use a 4.1-nano to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "('Layers are the building blocks of any image in Photoshop CC. You can think '\n",
      " 'of layers like separate flat panes of glass stacked on top of each other. '\n",
      " 'Each layer contains separate pieces of content. Some parts of a layer can be '\n",
      " 'transparent, allowing you to see through to the layers below. This setup '\n",
      " 'lets you edit parts of an image independently without affecting the rest of '\n",
      " 'the image. You manage and work with layers in the Layers panel, where you '\n",
      " 'can toggle their visibility on and off using the Eye icon. (See explanation '\n",
      " 'around 0:28 to 1:00 and 1:25 to 2:32) üé®üñºÔ∏è\\n'\n",
      " 'References:\\n'\n",
      " '[\\n'\n",
      " '  {\\n'\n",
      " '    \"title\": \"Understand layers\",\\n'\n",
      " '    \"source\": '\n",
      " '\"https://images-tv.adobe.com/avp/vr/b758b4c4-2a74-41f4-8e67-e2f2eab83c6a/f810fc5b-2b04-4e23-8fa4-5c532e7de6f8/e268fe4d-e5c7-415c-9f5c-d34d024b14d8_20170727011753.1280x720at2400_h264.mp4\",\\n'\n",
      " '    \"start\": 0.47,\\n'\n",
      " '    \"stop\": 62.14\\n'\n",
      " '  },\\n'\n",
      " '  {\\n'\n",
      " '    \"title\": \"Understand layers\",\\n'\n",
      " '    \"source\": '\n",
      " '\"https://images-tv.adobe.com/avp/vr/b758b4c4-2a74-41f4-8e67-e2f2eab83c6a/f810fc5b-2b04-4e23-8fa4-5c532e7de6f8/e268fe4d-e5c7-415c-9f5c-d34d024b14d8_20170727011753.1280x720at2400_h264.mp4\",\\n'\n",
      " '    \"start\": 85.75,\\n'\n",
      " '    \"stop\": 152.97\\n'\n",
      " '  }\\n'\n",
      " ']')\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "form_context = RunnableLambda(itemgetter(\"question\")) | {\n",
    "    \"context\":  retriever, \n",
    "    \"question\": RunnablePassthrough() \n",
    "    } \n",
    "\n",
    "answer_chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "get_videos =   form_context | \\\n",
    "    {\"input\":RunnablePassthrough(),\"answer\": answer_chain} |\\\n",
    "    RunnableLambda( lambda d: \n",
    "        {**d[\"input\"], \"answer\": d[\"answer\"] + \n",
    "         \"\\nReferences:\\n\" +\n",
    "         compile_references(d[\"input\"][\"context\"]) \n",
    "        } )\n",
    "    \n",
    "\n",
    "\n",
    "val = get_videos.invoke({\"question\":\"What are layers\"})\n",
    "pp(val[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Layers are the building blocks of any image in Photoshop CC. You can think '\n",
      " 'of layers like separate flat panes of glass stacked on top of each other. '\n",
      " 'Each layer contains separate pieces of content. Some parts of a layer can be '\n",
      " 'transparent, allowing you to see through to the layers below. This setup '\n",
      " 'lets you edit parts of an image independently without affecting the rest of '\n",
      " 'the image. You manage and work with layers in the Layers panel, where you '\n",
      " 'can toggle their visibility on and off using the Eye icon. (See explanation '\n",
      " 'around 0:28 to 1:00 and 1:25 to 2:32) üé®üñºÔ∏è\\n'\n",
      " 'References:\\n'\n",
      " '[\\n'\n",
      " '  {\\n'\n",
      " '    \"title\": \"Understand layers\",\\n'\n",
      " '    \"source\": '\n",
      " '\"https://images-tv.adobe.com/avp/vr/b758b4c4-2a74-41f4-8e67-e2f2eab83c6a/f810fc5b-2b04-4e23-8fa4-5c532e7de6f8/e268fe4d-e5c7-415c-9f5c-d34d024b14d8_20170727011753.1280x720at2400_h264.mp4\",\\n'\n",
      " '    \"start\": 0.47,\\n'\n",
      " '    \"stop\": 62.14\\n'\n",
      " '  },\\n'\n",
      " '  {\\n'\n",
      " '    \"title\": \"Understand layers\",\\n'\n",
      " '    \"source\": '\n",
      " '\"https://images-tv.adobe.com/avp/vr/b758b4c4-2a74-41f4-8e67-e2f2eab83c6a/f810fc5b-2b04-4e23-8fa4-5c532e7de6f8/e268fe4d-e5c7-415c-9f5c-d34d024b14d8_20170727011753.1280x720at2400_h264.mp4\",\\n'\n",
      " '    \"start\": 85.75,\\n'\n",
      " '    \"stop\": 152.97\\n'\n",
      " '  }\\n'\n",
      " ']')\n"
     ]
    }
   ],
   "source": [
    "pp(val[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Layers are the building blocks of any image in Photoshop CC. üñºÔ∏è They can be '\n",
      " 'thought of as separate flat pints of glass, stacked one on top of the other. '\n",
      " 'Each layer contains separate pieces of content, and some layers may have '\n",
      " 'transparent areas that let you see through to the layers below. The Layers '\n",
      " 'panel is where you select and work with layers, and you can toggle their '\n",
      " 'visibility by clicking the Eye icon. The main benefit of layers is that they '\n",
      " 'allow you to edit parts of an image independently without affecting the '\n",
      " 'rest. \\n'\n",
      " '\\n'\n",
      " 'üì∫ Watch the full explanation in the video titled \"Understand layers\" here: '\n",
      " '[https://images-tv.adobe.com/avp/vr/b758b4c4-2a74-41f4-8e67-e2f2eab83c6a/f810fc5b-2b04-4e23-8fa4-5c532e7de6f8/e268fe4d-e5c7-415c-9f5c-d34d024b14d8_20170727011753.1280x720at2400_h264.mp4](https://images-tv.adobe.com/avp/vr/b758b4c4-2a74-41f4-8e67-e2f2eab83c6a/f810fc5b-2b04-4e23-8fa4-5c532e7de6f8/e268fe4d-e5c7-415c-9f5c-d34d024b14d8_20170727011753.1280x720at2400_h264.mp4)')\n"
     ]
    }
   ],
   "source": [
    "pp(value.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[145]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     context: List[Document]\n\u001b[32m     11\u001b[39m     response: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m graph_builder = StateGraph(State).add_sequence([retrieve, \u001b[43mgenerate\u001b[49m ])\n\u001b[32m     14\u001b[39m graph_builder.add_edge(START, \u001b[33m\"\u001b[39m\u001b[33mretrieve\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m graph = graph_builder.compile()\n",
      "\u001b[31mNameError\u001b[39m: name 'generate' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict,Annotated\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import operator\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "        \n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate ])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "response = graph.invoke({\"question\" : \"What is the layer in Photoshop\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
