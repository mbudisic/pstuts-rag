{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstuts_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "def set_api_key_if_not_present(key_name, prompt_message=\"\"):\n",
    "    if len(prompt_message) == 0:\n",
    "        prompt_message=key_name\n",
    "    if key_name not in os.environ or not os.environ[key_name]:\n",
    "        os.environ[key_name] = getpass.getpass(prompt_message)\n",
    "\n",
    "set_api_key_if_not_present(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "First, we will read in the transcripts of the videos and convert them to Documents\n",
    "with appropriate metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"../data/test.json\"\n",
    "\n",
    "data = json.load(open(filename, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from pstuts_rag.datastore import transcripts_load\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "docs_chunks_semantic = transcripts_load(data,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R - retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hit it with a semantic chunker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "collection_name = f\"{filename}_qdrant\"\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(documents=docs_chunks_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "def retrieve(state):\n",
    "    retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "    return {\"context\":retrieved_docs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = retrieve({\"question\":\"What is a layer?\"})\n",
    "[ pp(d.page_content) for d in a[\"context\"] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - Augmentation\n",
    "\n",
    "We need to populate a prompt for LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful an expert on Photoshop and your goal is to help users\n",
    "gain knowledge from a database of training videos. \n",
    "You answer questions based on provided context. \n",
    "Your answers use emojis for emphasis.\n",
    "\n",
    "IMPORTANT: You must only use the provided context, and cannot use your own knowledge.\n",
    "If there is no context that corresponds to the query, respond by saying\n",
    "\"I don't know. This is not available in our training library.\"\n",
    "\n",
    "Most of the users questions will be in the form:\n",
    "\"How can I do ...\"\n",
    "or\n",
    "\"What is ...\"\n",
    "\n",
    "When appropriate, provide your answers in a step-by-step form.\n",
    "ALWAYS list the URL and the title of the reference video.\n",
    "NEVER invent the explanation. ALWAYS use ONLY the context information.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "RAG_PROMPT=\"\"\"\\\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "NEVER invent the explanation. ALWAYS use ONLY the context information.\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate(\n",
    "    [(\"system\",SYSTEM_PROMPT), \n",
    "     (\"human\",RAG_PROMPT)\n",
    "     ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "We will use a 4.1-nano to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "  docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "\n",
    "  references = [ \n",
    "                {k: doc.metadata[k] for k in (\"title\",\"source\",\"start\",\"stop\")} \n",
    "                for doc in state[\"context\"] \n",
    "  ] \n",
    "\n",
    "\n",
    "  messages = rag_prompt.format_messages(question=state[\"question\"], \n",
    "                                        context=docs_content)\n",
    "  response = llm.invoke(messages)\n",
    "  retval = {\"response\":f\"{response.content}\\n\\n**References**:\\n{json.dumps(references,indent=2)}\",\n",
    "            \"context\":state[\"context\"]}\n",
    "  \n",
    "  return retval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict,Annotated\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import operator\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "        \n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate ])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "response = graph.invoke({\"question\" : \"What is the layer in Photoshop\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
